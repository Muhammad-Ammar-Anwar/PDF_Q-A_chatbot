{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6312e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loaderpdf=PyPDFLoader('A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf')\n",
    "docs=loaderpdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93361159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large\\nmodels on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3\\nwith 175 billion parameters, fully fine-tuning them becomes computa-\\ntionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.\\nThis leads to up to 10,000 times fewer trainable parameters and reduces\\nGPU memory usage by three times. LoRA not only maintains but some-\\ntimes surpasses fine-tuning performance on models like RoBERTa, De-\\nBERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\\nno extra latency during inference, making it more efficient for practical\\napplications. All relevant code and model checkpoints are available at\\nhttps://github.com/microsoft/LoRA.\\n1 Introduction\\nMany natural language processing (NLP) applications rely on adapting large,\\npre-trained language models for various downstream tasks. Typically, this is\\ndone through fine-tuning, where all the parameters of the pre-trained model are\\nupdated. However, a significant drawback of fine-tuning is that the adapted\\nmodel has just as many parameters as the original one. As models grow in size,\\nwhat was once a manageable issue for models like GPT-2 or RoBERTa large\\nbecomes a serious deployment challenge with larger models like GPT-3, which\\nhas 175 billion trainable parameters.\\nTo mitigate these challenges, researchers have explored adapting only cer-\\ntain parts of the model or adding external modules specific to each task. This\\napproach reduces the need to store and manage large numbers of parameters\\nfor each task, greatly improving efficiency during deployment. However, cur-\\nrent methods often introduce drawbacks, such as inference delays by increasing\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='model depth or reducing the usable sequence length. Furthermore, these meth-\\nods typically do not perform as well as full fine-tuning, leading to a trade-off\\nbetween efficiency and model performance.\\nInspired by prior works that demonstrate over-parametrized models often\\nreside in a low intrinsic dimensional space, we hypothesize that weight changes\\nduring model adaptation also have a low “intrinsic rank.” This insight leads to\\nour Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\\nposition matrices for the dense layers’ weight changes during adaptation, while\\nkeeping the pre-trained weights frozen. As illustrated in Figure 1, even with\\nlarge models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\\nmatrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\\nstorage and computation.\\nLoRA has several notable advantages:\\n• The pre-trained model can be shared, and small LoRA modules can be\\ncreated for various tasks. By freezing the main model and only switching\\nthe matrices A and B (shown in Figure 1), storage and task-switching\\noverhead are significantly reduced.\\n• LoRA improves training efficiency and reduces hardware requirements,\\nlowering the entry barrier by up to threefold when using adaptive opti-\\nmizers. This is because LoRA only requires updating the smaller low-rank\\nmatrices, avoiding the need to calculate gradients for most parameters.\\n• The simple linear design allows merging of the trainable matrices with\\nthe frozen pre-trained weights during deployment, ensuring no additional\\ninference latency compared to fully fine-tuned models.\\n• LoRA is compatible with many existing methods and can be combined\\nwith approaches like prefix-tuning.\\nIn this work, we follow standard conventions for Transformer architecture\\nand refer to dimensions such asdmodel, and projection matrices likeWq, Wk, Wv,\\nand Wo for the self-attention module. W or W0 represents a pre-trained weight\\nmatrix, while ∆ W refers to its update during adaptation. The rank r denotes\\nthe rank of a LoRA module. Throughout, we use Adam for optimization and\\nmaintain the Transformer MLP feedforward dimension as dffn = 4 × dmodel.\\n1.1 Key Advantages of LoRA\\n• Efficient Task Switching : A pre-trained model can support multiple\\ntasks by swapping the small LoRA matrices, reducing storage needs.\\n• Reduced Hardware Requirements : LoRA lowers the GPU memory\\nneeded for training by freezing most parameters and only training the\\nlow-rank matrices.\\n• No Additional Latency: LoRA incurs no extra inference delay because\\nthe matrices can be merged with the pre-trained weights when deployed.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='• Combining with Other Methods : LoRA can be used with other ap-\\nproaches, like prefix-tuning, to further optimize model performance.\\n2 Problem Statement\\nAlthough our approach is independent of the specific training objective, we focus\\non language modeling as the central application. Below, we outline the key\\naspects of the language modeling problem, particularly the goal of maximizing\\nconditional probabilities based on task-specific prompts.\\nAssume we have an autoregressive language model PΦ(y|x) that is pre-\\ntrained and parameterized by Φ. For example, PΦ(y|x) could be a general\\nmulti-task model such as GPT, built on top of the Transformer architecture.\\nThe model can then be adapted to different downstream tasks such as text\\nsummarization, machine reading comprehension (MRC), and natural language\\nto SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:\\nZ = {(xi, yi)}i=1,...,N ,\\nwhere both xi and yi are sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yi would be the corresponding\\nSQL query; in summarization, xi represents the article and yi would be its\\nsummary.\\nIn traditional fine-tuning, the model is initialized using the pre-trained weights\\nΦ0, which are then updated to Φ 0 + ∆Φ by optimizing the model’s parameters\\nto maximize the conditional probabilities for each token:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|X\\nt=1\\nlog (PΦ(yt|x, y<t)) (1)\\nA significant limitation of full fine-tuning is that for every downstream task, a\\ndifferent set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the\\nsize of Φ0. For large models, such as GPT-3 with 175 billion parameters, storing\\nand deploying multiple instances of fine-tuned models becomes impractical or\\nextremely challenging.\\nTo address this issue, we propose a more efficient approach where the task-\\nspecific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\\nof parameters Θ, where |Θ| ≪ |Φ0|. As a result, optimizing the model for each\\ntask reduces to optimizing Θ as follows:\\nmax\\nΘ\\nX\\n(x,y)∈Z\\n|y|X\\nt=1\\nlog\\n\\x00\\npΦ0+∆Φ(Θ)(yt|x, y<t)\\n\\x01\\n(2)\\nIn the following sections, we explore a low-rank approach for representing\\n∆Φ, making the adaptation process more efficient in both computational and\\nmemory terms. For large models like GPT-3 175B, this method allows the\\ntrainable parameters |Θ| to be reduced to as little as 0.01% of |Φ0|.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='3 Limitations on Current Solutions\\nThe challenge we aim to address is not new. Since the rise of transfer learning,\\na great deal of work has focused on making model adaptation more efficient in\\nterms of both parameters and computation. For an overview, see Section 6 for\\nsome well-known works. Focusing on language modeling, two prominent strate-\\ngies for efficient adaptation stand out: adding adapter layers, or optimizing the\\ninput layer activations. However, both approaches come with limitations, espe-\\ncially when applied in large-scale, latency-sensitive production environments.\\n3.1 Adapter Layers and Inference Latency\\nThere are many variations of adapters. We focus on the original adapter design\\nfrom [ ?], which introduces two adapter layers per Transformer block, and a\\nmore recent approach by [?], which only uses one adapter per block but with an\\nadditional LayerNorm [?]. Although overall latency can be reduced by pruning\\nlayers or leveraging multi-task settings [ ?], [ ?], there is no way to completely\\neliminate the additional computation introduced by adapter layers. This might\\nseem minor since adapters generally have few parameters (typically less than\\n1% of the original model) due to their small bottleneck dimension, which limits\\nthe number of floating-point operations (FLOPs). However, large-scale neural\\nnetworks rely heavily on parallel processing to maintain low latency, and adapter\\nlayers are processed sequentially. This becomes more evident in scenarios with\\nlow batch sizes, such as real-time inference, where models like GPT-2 [?] running\\non a single GPU experience noticeable increases in latency, even with small\\nbottleneck dimensions (Table 1).\\nThe issue is further compounded when models need to be sharded across\\nmultiple devices, since the increased model depth requires more synchronous\\nGPU operations like AllReduce and Broadcast, unless adapter parameters are\\nredundantly replicated.\\n3.2 Challenges with Directly Optimizing the Prompt\\nAnother approach, such as prefix tuning [ ?], faces a different challenge. We\\nhave observed that prefix tuning is often difficult to optimize and that its per-\\nformance does not consistently improve as more trainable parameters are added,\\nconfirming earlier findings. Moreover, allocating part of the sequence length for\\nadaptation inevitably reduces the available sequence length for processing task-\\nrelated data, which seems to hinder prompt tuning’s performance compared to\\nother methods. We will further explore this issue in Section 5.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Batch Size Sequence Length |Θ| Latency (ms)\\nFine-Tune/LoRA\\n32 512 0.5M 1449.4 ± 0.8\\n16 256 11M 338.0 ± 0.6\\n1 128 11M 19.8 ± 2.7\\nAdapterL\\n32 512 0.5M 1482.0 ± 1.0 (+2.2%)\\n16 256 11M 354.8 ± 0.5 (+5.0%)\\n1 128 11M 23.9 ± 2.1 (+20.7%)\\nAdapterH\\n32 512 0.5M 1492.2 ± 1.0 (+3.0%)\\n16 256 11M 366.3 ± 0.5 (+8.4%)\\n1 128 11M 25.8 ± 2.2 (+30.3%)\\nTable 1: Inference latency of a forward pass in GPT-2 Medium measured over\\n100 trials using an NVIDIA Quadro RTX8000. ” |Θ|” refers to the number of\\ntrainable parameters in the adapter layers. Adapter L and AdapterH are two\\ntypes of adapter tuning. The impact on latency becomes significant, particularly\\nin online scenarios with shorter sequences and smaller batch sizes.\\n4 Our Method\\nIn this section, we explain the structure of LoRA and its practical benefits.\\nThe principles outlined here apply generally to dense layers in neural networks,\\nalthough we focus on specific weights in Transformer language models, as these\\nmodels serve as the central example in our experiments.\\n4.1 Low-Rank Parameterized Update Matrices\\nNeural networks contain numerous dense layers that perform matrix multipli-\\ncation, and the weight matrices in these layers typically have a full rank. When\\nadapting to a particular task, it shows that pre-trained language models pos-\\nsess a low ”intrinsic dimension” and can still perform effectively after a random\\nprojection to a smaller subspace. Drawing inspiration from this, we hypothesize\\nthat updates to the weights during adaptation also have a low ”intrinsic rank.”\\nFor a pre-trained weight matrix W0 ∈ Rd×k, we limit its update by express-\\ning it as a low-rank decomposition, W0 + ∆W = W0 + BA, where B ∈ Rd×r,\\nA ∈ Rr×k, and the rank r ≪ min(d, k). During training, W0 is fixed, and A\\nand B are the trainable parameters. Both W0 and ∆W = BA are multiplied\\nwith the input, and their respective outputs are summed element-wise. Thus,\\nfor h = W0x, our updated forward pass becomes:\\nh = W0x + ∆W x= W0x + BAx\\nWe illustrate this reparametrization in Figure 1. We initializeA with random\\nGaussian values and set B to zero, meaning ∆ W = BA is zero at the start\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='of training. We then scale ∆ W xby α\\nr , where α is a constant dependent on\\nr. When using Adam for optimization, adjusting α has an effect similar to\\ntuning the learning rate. Therefore, we use the same α for our first experiments\\nand avoid tuning it. This scaling method also minimizes the need to adjust\\nhyperparameters when varying r.\\n4.1.1 A Generalization of Full Fine-Tuning\\nA more general fine-tuning technique involves training only a subset of pre-\\ntrained parameters. LoRA extends this approach by eliminating the need for\\nfull-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\\nmatrices for adaptation. If LoRA is applied to all weight matrices, and all\\nbiases are trained, the expressiveness of full fine-tuning is recovered by setting\\nthe LoRA rank r equal to the rank of the pre-trained weight matrices. As the\\nnumber of trainable parameters increases, LoRA approaches the full fine-tuning\\nperformance, while adapter-based techniques converge to simpler models that\\ncannot process long input sequences.\\n4.1.2 No Additional Inference Latency\\nWhen deploying LoRA, we can explicitly compute W = W0 + BA and use it\\nduring inference. This means that when switching between tasks, we can quickly\\nsubtract BA and add a different low-rank matrix B′A′ without consuming ex-\\ntra memory. This ensures that no additional inference latency is introduced\\ncompared to fully fine-tuned models.\\n4.2 Applying LoRA to Transformer Models\\nIn principle, LoRA can be applied to any subset of weight matrices in a neural\\nnetwork to minimize the number of trainable parameters. In a Transformer\\narchitecture, the self-attention module contains four projection matrices Wq,\\nWk, Wv, and Wo, and the MLP module contains two more matrices. We treat\\nthe weight matrices in the self-attention module as a single dmodel × dmodel\\nmatrix, despite them being split into different attention heads. To simplify\\nthe process and improve parameter efficiency, we restrict our method to only\\nadapting the attention weights for downstream tasks, leaving the MLP module\\nfrozen. The effect of adapting various attention weight matrices in a Transformer\\nis further explored in Section 7.1. We leave the investigation of adapting MLP\\nlayers, LayerNorm layers, and biases to future work.\\n4.2.1 Practical Benefits and Limitations\\nOne of the major advantages of LoRA is its reduction in memory and storage\\ncosts. For large Transformer models using Adam, LoRA can cut VRAM usage\\nby up to two-thirds if r ≪ dmodel, since it eliminates the need to store optimizer\\nstates for frozen parameters. For example, with GPT-3 175B, VRAM usage\\nduring training drops from 1.2 TB to 350 GB. With r = 4, and only the query\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='and value matrices being adapted, the checkpoint size decreases by approxi-\\nmately 10,000× (from 350 GB to 35 MB). This makes it possible to train using\\nsignificantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier\\ntask-switching during deployment by simply swapping out the LoRA weights,\\nwhich requires far less memory than loading entirely new model parameters.\\nAdditionally, LoRA offers a 25% training speedup compared to full fine-tuning\\nbecause there is no need to compute gradients for most parameters.\\nHowever, LoRA does have some limitations. It is not straightforward to\\ncombine multiple tasks with different low-rank matrices A and B in a single\\nforward pass if BA is absorbed into W to remove additional inference latency.\\nWhile it is possible to dynamically select LoRA modules during inference, this\\nsolution is not suitable for scenarios where low-latency responses are crucial.\\n5 Empirical Experiments\\nWe assess LoRA’s performance in downstream tasks across several models in-\\ncluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\\nOur experiments cover various tasks, ranging from natural language understand-\\ning (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\\nwe evaluate on the GLUE benchmark. All experiments were performed using\\nNVIDIA Tesla V100 GPUs.\\n5.1 Baselines\\nFor comparison with a wide range of baselines, we replicate experimental setups\\nfrom previous studies and, where possible, reuse reported results. This might\\nresult in some baselines being present in only a subset of experiments.\\nFine-Tuning (FT) is a common method for adapting models. During fine-\\ntuning, the model’s pre-trained weights and biases are updated using gradient\\ndescent. A variant of this is fine-tuning only select layers, while freezing the\\nrest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,\\nwhile all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [ ?].\\nPrefix-embedding tuning (PreEmbed)involves adding special tokens to\\nthe input sequence, and training their embeddings. These tokens do not belong\\nto the model’s original vocabulary. Their placement—either prepended (prefix)\\nor appended (infix)—can significantly affect performance, as highlighted in [ ?].\\nPrefix-layer tuning (PreLayer) extends prefix tuning by learning train-\\nable activations at each Transformer layer. This results in a larger number of\\ntrainable parameters, as activations from prior layers are progressively replaced.\\nThe total number of trainable parameters is given by |Θ| = L×dmodel ×(lp +li),\\nwhere L is the number of Transformer layers.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Adapter tuning [?] introduces additional fully connected adapter layers\\nbetween existing layers in the Transformer. Several variants exist, such as\\nAdapterH and AdapterL [ ?], which differ in the placement of adapters within\\nthe network. The number of trainable parameters in these methods is |Θ| =\\nLAdpt × (2 × dmodel × r + r + dmodel) + 2× LLN × dmodel.\\nLoRA, on the other hand, introduces trainable low-rank matrices to the ex-\\nisting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank r and the shape of the original weight matrices:\\n|Θ| = 2 × LLoRA × dmodel × r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.\\nTable 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher\\nscores are better for all metrics. Confidence intervals are provided for experi-\\nments we conducted. *Results from prior work.\\nModel & Method# Trainable ParametersBLEU NIST MET ROUGE-LCIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47GPT-2 M (LoRA) 0.35M 70.4±0.1 8.85±0.2 46.8±0.2 71.8±0.1 2.53±0.2GPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45GPT-2 L (LoRA) 0.77M 70.4±0.1 8.89±0.2 46.8±0.2 72.0±0.2 2.47±0.2\\n5.2 Scaling LoRA to GPT-3 175B\\nTo further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\\ncomputational cost of GPT-3, we only report standard deviations for each task\\nbased on multiple random seeds. See Appendix D.4 for hyperparameters used.\\nAs presented in Table ??, LoRA matches or outperforms full fine-tuning on\\nWikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\\ndo not consistently benefit from increasing the number of trainable parame-\\nters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\\nthe performance degradation seen with larger token embeddings in prefix-based\\nmethods.\\nFigure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-\\nters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\\nbetter scalability and performance.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='6 Related Works\\n6.1 Transformer Language Models\\nThe Transformer architecture, as introduced by Vaswani et al. (2017), has\\nproven to be a highly effective sequence-to-sequence model due to its heavy use\\nof self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\\nlanguage modeling, significantly boosting its utility in the field. Since then,\\nTransformer-based models have become a staple in natural language processing\\n(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\\nway for large-scale pre-trained language models that, when fine-tuned, deliver\\nexcellent performance on specific tasks. The next breakthrough came with GPT-\\n3 (Brown et al., 2020), which is currently the largest single Transformer language\\nmodel with 175 billion parameters.\\n6.2 Prompt Engineering and Fine-Tuning\\nDespite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\\ning), its performance is highly sensitive to how the input prompt is structured\\n(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process\\nthat involves crafting and fine-tuning the input prompts to maximize model per-\\nformance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin\\net al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing\\nfull fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\\nposes significant challenges due to the large memory requirements and the com-\\nputational resources needed, making it as resource-intensive as pre-training.\\n6.3 Parameter-Efficient Adaptation\\nMany techniques have been developed to address the inefficiency of full fine-\\ntuning by adapting only certain layers or introducing adapter modules. Houlsby\\net al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\\nadapter layers between existing layers in the network. These adapters allow for\\nparameter-efficient adaptation by learning only a small number of task-specific\\nparameters. Our method imposes a low-rank constraint on the weight updates,\\nensuring that learned weights can be merged with the main model weights dur-\\ning inference, thus introducing no additional latency, unlike the adapter layers.\\nA related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\\nproducts to parametrize the adapters, further improving parameter efficiency.\\nAdditionally, prompt optimization techniques, such as those proposed by Li and\\nLiang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\\noptimize the input tokens directly. However, these approaches typically reduce\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='the available sequence length for task processing. Our work can be combined\\nwith such methods for further gains in efficiency.\\n6.4 Low-Rank Structures in Deep Learning\\nLow-rank structures are prevalent in many machine learning problems, and sev-\\neral studies have explored imposing these constraints on deep models. Li et\\nal. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\\nlearning tasks have an intrinsic low-rank structure. For deep neural networks,\\nparticularly over-parameterized models, it has been shown that they often ex-\\nhibit low-rank properties after training (Oymak et al., 2019). Prior works, such\\nas those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),\\nhave explicitly imposed low-rank constraints during training to enhance model\\nefficiency. However, our approach differs in that we apply low-rank updates to\\nfrozen pre-trained models, making it highly effective for task-specific adapta-\\ntion. Neural networks with low-rank structures have been shown to outperform\\nclassical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\\n2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in\\nadversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\\nposed low-rank adaptation well-grounded in both theory and practice.\\n7 Analyzing Low-Rank Adaptations\\nIn light of the demonstrated benefits of LoRA, we aim to further explore the\\nattributes of low-rank adaptation as applied to various downstream tasks. The\\nlow-rank structure does not only reduce the hardware requirements for conduct-\\ning parallel experiments, but it also provides better insight into how adapted\\nweights align with pre-trained weights. Our focus lies on GPT-3 175B, where\\nwe managed to significantly reduce the number of trainable parameters (up to\\n10,000×) without sacrificing task performance.\\nIn this section, we address some key questions:\\n• 1) With a constrained parameter budget, which weight matrices should\\nbe adapted to achieve the best downstream task performance?\\n• 2) Is the adapted matrix ∆W truly rank-deficient, and if so, what rank is\\noptimal for practical use?\\n• 3) How is ∆W related to the pre-trained weights W? Does ∆ W exhibit\\nhigh correlation with W, and what is the comparative size of ∆ W to W?\\nThe answers to these questions provide valuable insights for optimizing pre-\\ntrained models for downstream tasks.\\n7.1 Selecting Optimal Weight Matrices for LoRA\\nTo optimize performance under a limited parameter budget, we explore adapting\\ndifferent weight matrices within the self-attention module of the Transformer.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2024-09-10T21:50:42+00:00', 'moddate': '2024-09-10T21:50:42+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\\n175B, using a rank r = 8 for one attention weight type or r = 4 for two types.\\nThe results are displayed in Table 3.\\nTable 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\\ndifferent attention weights in GPT-3, with a fixed number of trainable parame-\\nters.\\n# of Trainable Parameters = 18M Wq Wk Wv Wo Wq, Wv\\nRank r = 8 70.4 70.0 73.0 73.2 73.7\\nMultiNLI (±0.1%) 91.0 90.8 91.0 91.3 91.7\\n7.2 Determining the Ideal Rank for LoRA\\nTo analyze the impact of the rankr on task performance, we applied LoRA with\\nvarying ranks across different combinations of attention matrices. The results\\ncan be found in Table 4.\\nTable 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\\nWeight Type r = 1 r = 2 r = 4 r = 8 r = 64\\nWikiSQL (±0.5%) Wq 68.8 69.6 70.5 70.4 70.0\\nWikiSQL (Wq, Wv) 73.4 73.3 73.7 73.8 73.5\\nMultiNLI (±0.1%) Wq 90.7 90.9 91.1 90.7 90.7\\nMultiNLI (Wq, Wv) 91.3 91.4 91.3 91.6 91.4\\nThe results show that even at a small rank r = 1, LoRA performs well when\\nboth Wq and Wv are adapted. In contrast, adapting only Wq requires a higher\\nrank for optimal performance.\\n8 Conclusion\\nLoRA offers a highly efficient solution to the problem of adapting large language\\nmodels for downstream tasks. By freezing the majority of the model’s param-\\neters and training only small, low-rank matrices, LoRA achieves comparable\\nperformance to full fine-tuning while drastically reducing computational costs.\\nIts ability to scale to massive models like GPT-3 without sacrificing performance\\nhighlights its potential for widespread use.\\nFuture work could explore combining LoRA with other parameter-efficient\\nmethods or investigating more principled ways to select which weight matrices\\nto adapt. Additionally, further studies on the rank deficiency of pre-trained\\nweights could inspire new developments in efficient model adaptation.\\n11')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503b5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv('HF_TOKEN')\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a307b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5799c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "text_spliter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=30)\n",
    "split=text_spliter.split_documents(docs)\n",
    "vectorstore=FAISS.from_documents(documents=split,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83619ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023781783FE0>, search_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc63e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model_name='Llama3-8b-8192',groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4e44f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hi').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fec0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"act as a Q/A chatbot\"\n",
    "    \"answer concise and detailed.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5733d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain=create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain=create_retrieval_chain(retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645b1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain.invoke({\"input\":\"What this paper is talking about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b10456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper appears to be discussing a method for adapting pre-trained neural networks to new tasks in a more efficient and computationally feasible way. The authors propose a reparameterization of the weight updates in the network, which allows them to reduce the number of trainable parameters and make the adaptation process more efficient.\\n\\nSpecifically, the paper introduces a low-rank approach for representing the updates to the weight matrices in the network, which allows the authors to reduce the number of trainable parameters to as little as 0.01% of the original size. This is achieved by expressing the updates as a low-rank decomposition of the original weight matrix, and representing the trainable parameters as a small set of matrices and vectors.\\n\\nThe paper also discusses the benefits of this approach, including reduced computational requirements and memory usage, and presents experiments using pre-trained language models as an example.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873da49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
