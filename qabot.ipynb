{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad6e7ab",
   "metadata": {},
   "source": [
    "### TASK #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loaderpdf=PyPDFLoader('A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf')\n",
    "docs=loaderpdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a03ea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications. All relevant code an\n"
     ]
    }
   ],
   "source": [
    "full_text = \" \".join([doc.page_content for doc in docs])\n",
    "print(full_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5152a",
   "metadata": {},
   "source": [
    "### TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1f27d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 323, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "\\documentclass{article}\n",
      "\n",
      "   egin{document}\n",
      "\n",
      "    \\maketitle\n",
      "\n",
      "    \\section{Introduction}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "\\subsection{History of LLMs}\n",
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "\n",
      "\\subsection{Applications of LLMs}\n",
      "\n",
      "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latex_text = \"\"\"\n",
    "\n",
    "    \\documentclass{article}\n",
    "\n",
    "    \\begin{document}\n",
    "\n",
    "    \\maketitle\n",
    "\n",
    "    \\section{Introduction}\n",
    "\n",
    "    Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "\n",
    "    \\subsection{History of LLMs}\n",
    "\n",
    "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "\n",
    "\\subsection{Applications of LLMs}\n",
    "\n",
    "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "\n",
    "\\end{document}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=300,chunk_overlap=50)\n",
    "chunk=text_splitter.split_text(latex_text)\n",
    "for i, chunk in enumerate(chunk):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a661354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 323, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494b2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "\\documentclass{article}\n",
      "\n",
      "   egin{document}\n",
      "\n",
      "    \\maketitle\n",
      "\n",
      "    \\section{Introduction}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "\\subsection{History of LLMs}\n",
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "\n",
      "\\subsection{Applications of LLMs}\n",
      "\n",
      "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f95dc2",
   "metadata": {},
   "source": [
    "### TASK #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d566d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a754d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM\n",
    "def get_llm():\n",
    "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.5,\n",
    "    }\n",
    "    project_id = \"skills-network\"\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=project_id,\n",
    "        params=parameters,\n",
    "    )\n",
    "    return watsonx_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e80572c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding model\n",
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ddbe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for WatsonxEmbeddings\n  Value error, Did not find 'apikey' or 'token', please add an environment variable `WATSONX_APIKEY` or 'WATSONX_TOKEN' which contains it, or pass 'apikey' or 'token' as a named parameter. [type=value_error, input_value={'model_id': 'ibm/slate-1...: {'input_text': True}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Define the embedding model (assuming the function from your code)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embedding_model = \u001b[43mwatsonx_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. Embed the query\u001b[39;00m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mHow are you?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mwatsonx_embedding\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwatsonx_embedding\u001b[39m():\n\u001b[32m      3\u001b[39m     embed_params = {\n\u001b[32m      4\u001b[39m         EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: \u001b[32m3\u001b[39m,\n\u001b[32m      5\u001b[39m         EmbedTextParamsMetaNames.RETURN_OPTIONS: {\u001b[33m\"\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m      6\u001b[39m     }\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     watsonx_embedding = \u001b[43mWatsonxEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mibm/slate-125m-english-rtrvr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://us-south.ml.cloud.ibm.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskills-network\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m watsonx_embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\coursera\\myenv\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for WatsonxEmbeddings\n  Value error, Did not find 'apikey' or 'token', please add an environment variable `WATSONX_APIKEY` or 'WATSONX_TOKEN' which contains it, or pass 'apikey' or 'token' as a named parameter. [type=value_error, input_value={'model_id': 'ibm/slate-1...: {'input_text': True}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82637a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6411569",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=watsonx_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff2bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 embedding values: [0.00700386194512248, 0.010914130136370659, 0.08746249973773956, 0.08679928630590439, 0.026648469269275665]\n"
     ]
    }
   ],
   "source": [
    "query= \"How are you?\"\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "print(\"First 5 embedding values:\", embedding_vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b9777",
   "metadata": {},
   "source": [
    "### TASK #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38626f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
